{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1Ch4o0bMkeqEuA4MKqt4CmrVl1vOej4RP","authorship_tag":"ABX9TyNsRtFdc1D7Z1+f+iHBVdif"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AeKY7jxciw8U"},"outputs":[],"source":["!pip install tqdm"]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm\n","\n","\n","def ex_tag(sid, page):\n","    \"\"\"\n","    뉴스 분야(sid)와 페이지(page)를 입력하면 해당 기사 링크들을 리스트로 추출하는 함수\n","    \"\"\"\n","    # 1. URL 설정\n","    url = f\"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid}&page={page}\"\n","    headers = {\n","        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n","    }\n","    html = requests.get(url, headers=headers)\n","\n","    # 2. HTML 파싱\n","    soup = BeautifulSoup(html.text, \"lxml\")\n","\n","    # 3. 기사 목록에서 링크 추출\n","    tag_lst = []\n","    news_list = soup.select('.type06_headline li dl a')  # 기사 링크가 포함된 a 태그 선택\n","\n","    for a in news_list:\n","        href = a.get(\"href\")\n","        if href and \"article\" in href:\n","            tag_lst.append(href)\n","\n","    return tag_lst\n","def re_tag(sid):\n","    ### 특정 분야의 100페이지까지의 뉴스의 링크를 수집하여 중복 제거한 리스트로 변환하는 함수 ###\n","    re_lst = []\n","    for i in tqdm(range(100)):\n","        lst = ex_tag(sid, i+1)\n","        re_lst.extend(lst)\n","\n","    # 중복 제거\n","    re_set = set(re_lst)\n","    re_lst = list(re_set)\n","\n","    return re_lst\n","\n","all_hrefs = {}\n","sids = [101]  # 분야 리스트\n","\n","# 각 분야별로 링크 수집해서 딕셔너리에 저장\n","for sid in sids:\n","    sid_data = re_tag(sid)\n","    all_hrefs[sid] = sid_data"],"metadata":{"id":"wI2IX5vSrY9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_hrefs"],"metadata":{"id":"Q20ybcb1sF0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def art_crawl(all_hrefs, sid, index):\n","    \"\"\"\n","    sid와 링크 인덱스를 넣으면 기사제목, 날짜, 본문을 크롤링하여 딕셔너리를 출력하는 함수\n","\n","    Args:\n","        all_hrefs(dict): 각 분야별로 100페이지까지 링크를 수집한 딕셔너리 (key: 분야(sid), value: 링크)\n","        sid(int): 분야 [100: 정치, 101: 경제, 102: 사회, 103: 생활/문화, 104: 세계, 105: IT/과학]\n","        index(int): 링크의 인덱스\n","\n","    Returns:\n","        dict: 기사제목, 날짜, 본문이 크롤링된 딕셔너리\n","\n","    \"\"\"\n","    art_dic = {}\n","\n","    ## 1.\n","    title_selector = \"#title_area > span\"\n","    date_selector = \"#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans\"\\\n","    \"> div.media_end_head_info_datestamp > div:nth-child(1) > span\"\n","    main_selector = \"#dic_area\"\n","\n","    url = all_hrefs[101][index]\n","    html = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 \"\\\n","    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\\\n","    \"Chrome/130.0.0.0 Safari/537.36\"})\n","    soup = BeautifulSoup(html.text, \"lxml\")\n","\n","    ## 2.\n","    # 제목 수집\n","    title = soup.select(title_selector)\n","    title_lst = [t.text for t in title]\n","    title_str = \"\".join(title_lst)\n","\n","    # 날짜 수집\n","    # date = soup.select(date_selector)\n","    # date_lst = [d.text for d in date]\n","    # date_str = \"\".join(date_lst)\n","\n","    # 본문 수집\n","    main = soup.select(main_selector)\n","    main_lst = []\n","    for m in main:\n","        m_text = m.text\n","        m_text = m_text.strip()\n","        main_lst.append(m_text)\n","    main_str = \"\".join(main_lst)\n","\n","    ## 3.\n","    art_dic[\"title\"] = title_str\n","    # art_dic[\"date\"] = date_str\n","    art_dic[\"main\"] = main_str\n","\n","    return art_dic"],"metadata":{"id":"SLyZBD15rtC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["section_lst = [101]\n","artdic_lst = []\n","\n","for section in tqdm(section_lst):\n","    for i in tqdm(range(len(all_hrefs[section]))):\n","        art_dic = art_crawl(all_hrefs, section, i)\n","        art_dic[\"section\"] = section\n","        art_dic[\"url\"] = all_hrefs[section][i]\n","        artdic_lst.append(art_dic)\n"],"metadata":{"id":"SdnhMaZEsYIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["artdic_lst"],"metadata":{"id":"iLooPPu2spb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","def csv_to_custom_json(csv_file: str, json_file: str):\n","    \"\"\"\n","    CSV 파일에서 'title' 열을 추출하여 'relation_type'에 삽입하고 JSON 파일로 저장.\n","\n","    Args:\n","        csv_file (str): 원본 CSV 파일 경로.\n","        json_file (str): 저장할 JSON 파일 경로.\n","    \"\"\"\n","    try:\n","        # CSV 파일 읽기\n","        df = pd.read_csv(csv_file)\n","\n","        # 'title' 열 추출\n","        if 'title' not in df.columns:\n","            raise ValueError(\"'title' 열이 CSV 파일에 없습니다.\")\n","\n","        titles = df['title'].dropna().tolist()  # 'title' 열을 리스트로 변환\n","\n","        # JSON 형식 변환\n","        result = []\n","        for title in titles:\n","            result.append({\n","                \"relation_type\": title,  # 'title' 값을 relation_type에 삽입\n","                \"keywords\": [\"키워드1\", \"키워드2\", \"키워드3\"]  # 임의의 키워드\n","            })\n","\n","        # JSON 파일로 저장\n","        with open(json_file, 'w', encoding='utf-8') as f:\n","            json.dump(result, f, ensure_ascii=False, indent=4)\n","\n","        print(f\"JSON 파일로 저장되었습니다: {json_file}\")\n","    except Exception as e:\n","        print(f\"에러 발생: {e}\")\n","\n","# 사용 예제\n","csv_file_path = \"/article_df.csv\"  # 원본 CSV 파일 경로\n","json_file_path = \"/titles_with_relation_type.json\"  # 저장할 JSON 파일 경로\n","\n","csv_to_custom_json(csv_file_path, json_file_path)\n"],"metadata":{"id":"AWI43T7ZAiYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# CSV 파일을 불러오는 함수 (한글 파일 읽기 지원)\n","def load_training_articles(file_path):\n","    try:\n","        # UTF-8로 먼저 시도\n","        df = pd.read_csv(file_path, usecols=['title', 'main'], encoding='utf-8')\n","    except UnicodeDecodeError:\n","        # UTF-8로 읽지 못하면 EUC-KR로 재시도\n","        df = pd.read_csv(file_path, usecols=['title', 'main'], encoding='euc-kr')\n","\n","    # main 열에서 .com, co.kr, 전화번호 제거\n","    def clean_text(text):\n","        # .com, co.kr 제거\n","        text = re.sub(r'\\b\\w+\\.(com|co\\.kr)\\b', '', text)\n","        # 전화번호 형식 제거 (010-1234-5678, (02) 1234-5678 등)\n","        text = re.sub(r'\\b\\d{2,3}[-.\\)]?\\d{3,4}[-.]\\d{4}\\b', '', text)\n","        return text\n","\n","    # 정규 표현식을 통해 불필요한 부분 제거\n","    df['main'] = df['main'].apply(clean_text)\n","\n","    # title과 main을 결합하여 하나의 리스트로 생성\n","    training_articles = df.apply(lambda row: f\"{row['title']} {row['main']}\", axis=1).tolist()\n","\n","    return training_articles\n","\n","# 예시 파일 경로\n","file_path = 'article_df.csv'\n","\n","# training_articles 생성\n","training_articles = load_training_articles(file_path)\n","\n","# 결과 확인\n","for article in training_articles:\n","    print(\"Article:\", article)\n","    print()  # 각 기사 간 개행\n"],"metadata":{"id":"sw8B3tA6zENa"},"execution_count":null,"outputs":[]}]}