{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd6efd2-dc45-4c42-bbf8-f625b3525f9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sashi\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.0/9.7 MB 27.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.7 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.26.1 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1db9296-a40c-4fad-a6e8-aa5a1cb98013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력해주세요: IT\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력): 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링할 종료 페이지:  300 페이지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4434/4434 [00:00<00:00, 4268428.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1378/1378 [07:23<00:00,  3.11it/s]\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링된 데이터가 XLSX 형식으로 저장되었습니다: IT_20241123_193247.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&start={start_page}\"\n",
    "        return url\n",
    "    else:\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&start={page}\"\n",
    "            urls.append(url)\n",
    "        return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles, attrs):\n",
    "    attrs_content = []\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError 방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "# html 생성해서 기사 크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    original_html = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver, 'href')\n",
    "    return url\n",
    "\n",
    "# 뉴스 크롤링 시작\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \", page, \"페이지\")   \n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \", page2, \"페이지\")   \n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search, page, page2)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_urls = []\n",
    "news_contents = []\n",
    "news_dates = []\n",
    "\n",
    "for i in url:\n",
    "    urls = articles_crawler(i)\n",
    "    news_urls.append(urls)\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_urls)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title is None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    \n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"article#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html 태그 제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    # 날짜 가져오기\n",
    "    try:\n",
    "        html_date = news_html.select_one(\"div#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \", (page2 + 1 - page) * 10, '개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "print(news_contents)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "# 데이터 프레임으로 만들기\n",
    "news_df = pd.DataFrame({\n",
    "    'date': news_dates,\n",
    "    'title': news_titles,\n",
    "    'link': final_urls,\n",
    "    'content': news_contents\n",
    "})\n",
    "\n",
    "# 중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \", len(news_df))\n",
    "\n",
    "# 데이터 프레임 저장\n",
    "now = datetime.datetime.now()\n",
    "news_df.to_excel(f'{search}_{now.strftime(\"%Y%m%d_%H%M%S\")}.xlsx', index=False)\n",
    "\n",
    "print(f\"크롤링된 데이터가 XLSX 형식으로 저장되었습니다: {search}_{now.strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86e98a-a604-4fd0-aafb-549931696113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f5b19-ceaa-48bf-9657-ef93e321c08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8ef13-a80b-4bc6-b060-5b1293069180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
